{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 2: EM for Bayesian Networks\n",
    "### House Votes Dataset Analysis"
   ],
   "id": "104d9be2558787ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:44.398032Z",
     "start_time": "2025-05-06T18:00:44.139787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from itertools import permutations, product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "ef4eddc2cf5aa96f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "38a1a486881f8371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This loads the `house-votes-84.data` dataset and preprocesses it:\n",
    "1. **Load Data**: Reads the CSV file into a pandas DataFrame.\n",
    "2. **Feature Selection**: Keeps only the first three columns (party affiliation and two votes).\n",
    "3. **Column Renaming**: Labels columns as `party`, `vote1`, and `vote2` for clarity.\n",
    "4. **Categorical Encoding**: Converts `democrat`/`republican` to `0`/`1`, `y`/`n` votes to `1`/`0`, and maps missing votes (`?`) to `NaN`.\n",
    "5. **Train-Test Split**: Uses the first 300 rows for training and the remainder for testing.\n",
    "This prepares the data for training Bayesian network models using the EM algorithm while handling missing values."
   ],
   "id": "350700bc71f39f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:45.914799Z",
     "start_time": "2025-05-06T18:00:45.887186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('house-votes-84.data', header=None)\n",
    "data = data.iloc[:, :3]\n",
    "data.columns = ['party', 'vote1', 'vote2']\n",
    "\n",
    "data['party'] = data['party'].map({'democrat': 0, 'republican': 1})\n",
    "for col in ['vote1', 'vote2']:\n",
    "    data[col] = data[col].map({'y': 1, 'n': 0, '?': np.nan})\n",
    "\n",
    "train_data = data.iloc[:300]\n",
    "test_data = data.iloc[300:]"
   ],
   "id": "80dadcd99873ae09",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:46.352030Z",
     "start_time": "2025-05-06T18:00:46.328808Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_data.to_markdown())",
   "id": "13a5e9fba6b557fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |   party |   vote1 |   vote2 |\n",
      "|----:|--------:|--------:|--------:|\n",
      "|   0 |       1 |       0 |       1 |\n",
      "|   1 |       1 |       0 |       1 |\n",
      "|   2 |       0 |     nan |       1 |\n",
      "|   3 |       0 |       0 |       1 |\n",
      "|   4 |       0 |       1 |       1 |\n",
      "|   5 |       0 |       0 |       1 |\n",
      "|   6 |       0 |       0 |       1 |\n",
      "|   7 |       1 |       0 |       1 |\n",
      "|   8 |       1 |       0 |       1 |\n",
      "|   9 |       0 |       1 |       1 |\n",
      "|  10 |       1 |       0 |       1 |\n",
      "|  11 |       1 |       0 |       1 |\n",
      "|  12 |       0 |       0 |       1 |\n",
      "|  13 |       0 |       1 |       1 |\n",
      "|  14 |       1 |       0 |       1 |\n",
      "|  15 |       1 |       0 |       1 |\n",
      "|  16 |       0 |       1 |       0 |\n",
      "|  17 |       0 |       1 |     nan |\n",
      "|  18 |       1 |       0 |       1 |\n",
      "|  19 |       0 |       1 |       1 |\n",
      "|  20 |       0 |       1 |       1 |\n",
      "|  21 |       0 |       1 |       1 |\n",
      "|  22 |       0 |       1 |     nan |\n",
      "|  23 |       0 |       1 |       1 |\n",
      "|  24 |       0 |       1 |       0 |\n",
      "|  25 |       0 |       1 |       0 |\n",
      "|  26 |       0 |       1 |       0 |\n",
      "|  27 |       0 |       1 |       1 |\n",
      "|  28 |       1 |       1 |       0 |\n",
      "|  29 |       0 |       1 |       1 |\n",
      "|  30 |       1 |       0 |       1 |\n",
      "|  31 |       0 |       1 |       1 |\n",
      "|  32 |       0 |       1 |       1 |\n",
      "|  33 |       1 |       0 |       1 |\n",
      "|  34 |       0 |       1 |       1 |\n",
      "|  35 |       1 |       0 |       1 |\n",
      "|  36 |       1 |       1 |     nan |\n",
      "|  37 |       1 |       1 |       1 |\n",
      "|  38 |       1 |       0 |       1 |\n",
      "|  39 |       0 |       1 |       0 |\n",
      "|  40 |       0 |       1 |       1 |\n",
      "|  41 |       0 |       1 |       1 |\n",
      "|  42 |       0 |       1 |       0 |\n",
      "|  43 |       0 |       1 |       0 |\n",
      "|  44 |       0 |       1 |       1 |\n",
      "|  45 |       0 |       1 |       1 |\n",
      "|  46 |       0 |       1 |       1 |\n",
      "|  47 |       0 |       1 |       0 |\n",
      "|  48 |       0 |       1 |       1 |\n",
      "|  49 |       1 |       0 |     nan |\n",
      "|  50 |       0 |       1 |       1 |\n",
      "|  51 |       1 |       0 |       1 |\n",
      "|  52 |       0 |       1 |       1 |\n",
      "|  53 |       1 |       1 |       1 |\n",
      "|  54 |       0 |       1 |       1 |\n",
      "|  55 |       1 |       0 |       1 |\n",
      "|  56 |       1 |       0 |       1 |\n",
      "|  57 |       1 |       0 |       1 |\n",
      "|  58 |       1 |       0 |       1 |\n",
      "|  59 |       1 |       0 |       1 |\n",
      "|  60 |       0 |       1 |       1 |\n",
      "|  61 |       1 |       0 |       1 |\n",
      "|  62 |       0 |       1 |       1 |\n",
      "|  63 |       0 |       1 |       1 |\n",
      "|  64 |       0 |       1 |       1 |\n",
      "|  65 |       1 |       1 |       1 |\n",
      "|  66 |       1 |       0 |       1 |\n",
      "|  67 |       1 |       0 |       1 |\n",
      "|  68 |       0 |       1 |     nan |\n",
      "|  69 |       0 |       1 |       1 |\n",
      "|  70 |       0 |       1 |       0 |\n",
      "|  71 |       1 |       1 |       1 |\n",
      "|  72 |       0 |       1 |       1 |\n",
      "|  73 |       1 |       1 |       0 |\n",
      "|  74 |       0 |       1 |       0 |\n",
      "|  75 |       0 |       0 |       1 |\n",
      "|  76 |       0 |       0 |       1 |\n",
      "|  77 |       0 |       0 |       1 |\n",
      "|  78 |       0 |       1 |       1 |\n",
      "|  79 |       1 |       0 |       0 |\n",
      "|  80 |       0 |       1 |       0 |\n",
      "|  81 |       0 |       1 |       0 |\n",
      "|  82 |       1 |       0 |       0 |\n",
      "|  83 |       1 |       0 |       0 |\n",
      "|  84 |       1 |       0 |     nan |\n",
      "|  85 |       0 |       0 |       0 |\n",
      "|  86 |       1 |       0 |       0 |\n",
      "|  87 |       1 |       0 |       0 |\n",
      "|  88 |       0 |       0 |       1 |\n",
      "|  89 |       1 |       0 |       0 |\n",
      "|  90 |       0 |       1 |       0 |\n",
      "|  91 |       0 |       1 |       0 |\n",
      "|  92 |       0 |       1 |       1 |\n",
      "|  93 |       0 |       1 |       0 |\n",
      "|  94 |       0 |       1 |       0 |\n",
      "|  95 |       0 |       1 |       0 |\n",
      "|  96 |       0 |       0 |       0 |\n",
      "|  97 |       0 |       1 |       0 |\n",
      "|  98 |       0 |       1 |       1 |\n",
      "|  99 |       1 |       0 |       0 |\n",
      "| 100 |       0 |       1 |       0 |\n",
      "| 101 |       0 |       1 |       0 |\n",
      "| 102 |       0 |       1 |       0 |\n",
      "| 103 |       0 |       1 |       0 |\n",
      "| 104 |       0 |     nan |     nan |\n",
      "| 105 |       0 |       1 |       1 |\n",
      "| 106 |       1 |       0 |       1 |\n",
      "| 107 |       1 |       0 |     nan |\n",
      "| 108 |       0 |       1 |     nan |\n",
      "| 109 |       0 |       1 |     nan |\n",
      "| 110 |       0 |       0 |       0 |\n",
      "| 111 |       1 |       0 |     nan |\n",
      "| 112 |       0 |       0 |     nan |\n",
      "| 113 |       1 |       0 |     nan |\n",
      "| 114 |       0 |       1 |     nan |\n",
      "| 115 |       0 |       0 |     nan |\n",
      "| 116 |       0 |       1 |       0 |\n",
      "| 117 |       1 |       1 |       1 |\n",
      "| 118 |       0 |       0 |       1 |\n",
      "| 119 |       1 |       0 |       0 |\n",
      "| 120 |       1 |       0 |     nan |\n",
      "| 121 |       1 |       0 |     nan |\n",
      "| 122 |       1 |       0 |       0 |\n",
      "| 123 |       1 |       1 |     nan |\n",
      "| 124 |       0 |       0 |     nan |\n",
      "| 125 |       1 |       0 |     nan |\n",
      "| 126 |       1 |       0 |     nan |\n",
      "| 127 |       0 |       0 |     nan |\n",
      "| 128 |       0 |       0 |     nan |\n",
      "| 129 |       0 |     nan |     nan |\n",
      "| 130 |       0 |       1 |     nan |\n",
      "| 131 |       0 |       0 |       0 |\n",
      "| 132 |       1 |       0 |       0 |\n",
      "| 133 |       1 |       0 |       0 |\n",
      "| 134 |       1 |       0 |       0 |\n",
      "| 135 |       1 |       0 |       0 |\n",
      "| 136 |       1 |       0 |       1 |\n",
      "| 137 |       0 |       0 |     nan |\n",
      "| 138 |       0 |       0 |       0 |\n",
      "| 139 |       0 |       1 |       0 |\n",
      "| 140 |       1 |       0 |       0 |\n",
      "| 141 |       1 |       0 |       0 |\n",
      "| 142 |       1 |       0 |       0 |\n",
      "| 143 |       0 |     nan |       1 |\n",
      "| 144 |       0 |       0 |     nan |\n",
      "| 145 |       0 |       0 |       0 |\n",
      "| 146 |       1 |       0 |       1 |\n",
      "| 147 |       0 |       0 |       0 |\n",
      "| 148 |       1 |       0 |       1 |\n",
      "| 149 |       0 |       0 |       0 |\n",
      "| 150 |       1 |       1 |       1 |\n",
      "| 151 |       0 |       1 |       1 |\n",
      "| 152 |       0 |       0 |       1 |\n",
      "| 153 |       0 |       0 |       0 |\n",
      "| 154 |       1 |       0 |       1 |\n",
      "| 155 |       1 |       1 |       1 |\n",
      "| 156 |       1 |       1 |       1 |\n",
      "| 157 |       0 |       0 |       1 |\n",
      "| 158 |       1 |       0 |       1 |\n",
      "| 159 |       0 |       0 |       1 |\n",
      "| 160 |       0 |       0 |       1 |\n",
      "| 161 |       0 |       0 |       0 |\n",
      "| 162 |       0 |       0 |       1 |\n",
      "| 163 |       1 |       0 |       1 |\n",
      "| 164 |       0 |       1 |       1 |\n",
      "| 165 |       0 |       0 |       1 |\n",
      "| 166 |       1 |       1 |       0 |\n",
      "| 167 |       1 |       1 |       0 |\n",
      "| 168 |       0 |       0 |       0 |\n",
      "| 169 |       0 |       1 |       0 |\n",
      "| 170 |       0 |       1 |       0 |\n",
      "| 171 |       1 |       0 |     nan |\n",
      "| 172 |       0 |       0 |       1 |\n",
      "| 173 |       0 |       0 |       0 |\n",
      "| 174 |       0 |       1 |     nan |\n",
      "| 175 |       0 |       0 |       1 |\n",
      "| 176 |       1 |       0 |       0 |\n",
      "| 177 |       0 |       0 |       0 |\n",
      "| 178 |       0 |     nan |       0 |\n",
      "| 179 |       0 |       1 |       0 |\n",
      "| 180 |       0 |     nan |     nan |\n",
      "| 181 |       0 |       0 |       0 |\n",
      "| 182 |       0 |       1 |     nan |\n",
      "| 183 |       0 |     nan |     nan |\n",
      "| 184 |       0 |       0 |       0 |\n",
      "| 185 |       0 |       1 |       0 |\n",
      "| 186 |       0 |       0 |       1 |\n",
      "| 187 |       0 |       1 |       0 |\n",
      "| 188 |       1 |       1 |     nan |\n",
      "| 189 |       0 |       1 |       0 |\n",
      "| 190 |       1 |       0 |     nan |\n",
      "| 191 |       1 |       0 |       1 |\n",
      "| 192 |       0 |       0 |       0 |\n",
      "| 193 |       0 |       0 |       0 |\n",
      "| 194 |       0 |       0 |       0 |\n",
      "| 195 |       1 |       0 |       0 |\n",
      "| 196 |       0 |       0 |       0 |\n",
      "| 197 |       1 |       0 |     nan |\n",
      "| 198 |       0 |       0 |       0 |\n",
      "| 199 |       0 |       1 |       1 |\n",
      "| 200 |       0 |       0 |       0 |\n",
      "| 201 |       0 |       1 |       1 |\n",
      "| 202 |       0 |       1 |       0 |\n",
      "| 203 |       0 |       1 |       0 |\n",
      "| 204 |       1 |       0 |       0 |\n",
      "| 205 |       0 |       0 |       0 |\n",
      "| 206 |       1 |       0 |       0 |\n",
      "| 207 |       1 |       1 |     nan |\n",
      "| 208 |       0 |       0 |       0 |\n",
      "| 209 |       0 |       1 |       1 |\n",
      "| 210 |       0 |       0 |       0 |\n",
      "| 211 |       1 |       0 |       1 |\n",
      "| 212 |       0 |       0 |       0 |\n",
      "| 213 |       0 |       0 |       1 |\n",
      "| 214 |       1 |       1 |       1 |\n",
      "| 215 |       0 |       0 |       1 |\n",
      "| 216 |       0 |       1 |       1 |\n",
      "| 217 |       1 |       0 |       1 |\n",
      "| 218 |       0 |       1 |     nan |\n",
      "| 219 |       0 |       0 |       1 |\n",
      "| 220 |       0 |       0 |       0 |\n",
      "| 221 |       0 |       0 |       1 |\n",
      "| 222 |       0 |       1 |       0 |\n",
      "| 223 |       1 |       0 |       0 |\n",
      "| 224 |       1 |       0 |       1 |\n",
      "| 225 |       1 |       0 |     nan |\n",
      "| 226 |       0 |       0 |       0 |\n",
      "| 227 |       0 |       1 |       0 |\n",
      "| 228 |       1 |       0 |       1 |\n",
      "| 229 |       1 |       0 |       1 |\n",
      "| 230 |       1 |       0 |       1 |\n",
      "| 231 |       1 |       0 |       1 |\n",
      "| 232 |       0 |       0 |       1 |\n",
      "| 233 |       1 |       0 |       1 |\n",
      "| 234 |       0 |       0 |       0 |\n",
      "| 235 |       1 |       0 |       0 |\n",
      "| 236 |       0 |       0 |       0 |\n",
      "| 237 |       0 |       1 |       0 |\n",
      "| 238 |       0 |       1 |       1 |\n",
      "| 239 |       1 |       0 |       0 |\n",
      "| 240 |       1 |       0 |       0 |\n",
      "| 241 |       0 |       1 |       0 |\n",
      "| 242 |       1 |       0 |       0 |\n",
      "| 243 |       0 |       0 |       1 |\n",
      "| 244 |       0 |       1 |       0 |\n",
      "| 245 |       0 |       1 |       0 |\n",
      "| 246 |       0 |       0 |       0 |\n",
      "| 247 |       1 |       0 |       1 |\n",
      "| 248 |       1 |     nan |     nan |\n",
      "| 249 |       0 |       1 |       0 |\n",
      "| 250 |       1 |       0 |       1 |\n",
      "| 251 |       1 |       0 |       1 |\n",
      "| 252 |       0 |       1 |       1 |\n",
      "| 253 |       1 |       0 |       1 |\n",
      "| 254 |       0 |       1 |       0 |\n",
      "| 255 |       0 |       1 |       0 |\n",
      "| 256 |       1 |       0 |       0 |\n",
      "| 257 |       1 |       0 |       0 |\n",
      "| 258 |       0 |       0 |       0 |\n",
      "| 259 |       0 |       1 |       0 |\n",
      "| 260 |       0 |       1 |       0 |\n",
      "| 261 |       0 |       1 |       0 |\n",
      "| 262 |       0 |       1 |       0 |\n",
      "| 263 |       0 |       1 |       0 |\n",
      "| 264 |       0 |       1 |       0 |\n",
      "| 265 |       0 |       1 |       0 |\n",
      "| 266 |       1 |       0 |       0 |\n",
      "| 267 |       1 |       1 |       0 |\n",
      "| 268 |       0 |       1 |       0 |\n",
      "| 269 |       0 |       1 |       0 |\n",
      "| 270 |       0 |       1 |       1 |\n",
      "| 271 |       0 |       0 |       1 |\n",
      "| 272 |       0 |       1 |       0 |\n",
      "| 273 |       1 |       0 |       0 |\n",
      "| 274 |       1 |       1 |       0 |\n",
      "| 275 |       0 |       0 |       0 |\n",
      "| 276 |       1 |       0 |       0 |\n",
      "| 277 |       1 |       0 |       0 |\n",
      "| 278 |       1 |       0 |       0 |\n",
      "| 279 |       1 |       0 |       0 |\n",
      "| 280 |       0 |       0 |       0 |\n",
      "| 281 |       1 |       1 |       0 |\n",
      "| 282 |       1 |       1 |       0 |\n",
      "| 283 |       1 |       0 |       0 |\n",
      "| 284 |       0 |       0 |       0 |\n",
      "| 285 |       0 |       0 |       0 |\n",
      "| 286 |       0 |       0 |       0 |\n",
      "| 287 |       0 |       1 |       1 |\n",
      "| 288 |       0 |       1 |       1 |\n",
      "| 289 |       0 |       1 |       1 |\n",
      "| 290 |       0 |       1 |       0 |\n",
      "| 291 |       0 |       1 |       0 |\n",
      "| 292 |       0 |       1 |       0 |\n",
      "| 293 |       0 |       1 |       1 |\n",
      "| 294 |       0 |       0 |       1 |\n",
      "| 295 |       1 |       0 |       1 |\n",
      "| 296 |       1 |       0 |       0 |\n",
      "| 297 |       0 |       1 |       1 |\n",
      "| 298 |       0 |       0 |       1 |\n",
      "| 299 |       0 |       0 |       1 |\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:46.850766Z",
     "start_time": "2025-05-06T18:00:46.843005Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_data.to_markdown())",
   "id": "c422e60e517d452b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |   party |   vote1 |   vote2 |\n",
      "|----:|--------:|--------:|--------:|\n",
      "| 300 |       1 |       0 |       0 |\n",
      "| 301 |       0 |       0 |       0 |\n",
      "| 302 |       1 |       0 |       0 |\n",
      "| 303 |       1 |       0 |       0 |\n",
      "| 304 |       1 |       0 |       1 |\n",
      "| 305 |       1 |       0 |       0 |\n",
      "| 306 |       1 |       0 |       0 |\n",
      "| 307 |       0 |       1 |       0 |\n",
      "| 308 |       1 |       0 |       0 |\n",
      "| 309 |       0 |       1 |       0 |\n",
      "| 310 |       1 |       0 |       0 |\n",
      "| 311 |       0 |       0 |       0 |\n",
      "| 312 |       0 |       1 |       1 |\n",
      "| 313 |       1 |       0 |       1 |\n",
      "| 314 |       1 |       0 |       1 |\n",
      "| 315 |       1 |       0 |       1 |\n",
      "| 316 |       0 |       0 |       0 |\n",
      "| 317 |       0 |       1 |       0 |\n",
      "| 318 |       0 |       0 |       0 |\n",
      "| 319 |       0 |       1 |       0 |\n",
      "| 320 |       0 |       0 |       1 |\n",
      "| 321 |       0 |       1 |       1 |\n",
      "| 322 |       0 |       1 |       1 |\n",
      "| 323 |       0 |       1 |       1 |\n",
      "| 324 |       1 |       0 |       1 |\n",
      "| 325 |       0 |       0 |       1 |\n",
      "| 326 |       0 |       1 |       1 |\n",
      "| 327 |       1 |       0 |       1 |\n",
      "| 328 |       0 |       1 |       1 |\n",
      "| 329 |       0 |       1 |     nan |\n",
      "| 330 |       1 |       0 |       1 |\n",
      "| 331 |       0 |       1 |     nan |\n",
      "| 332 |       0 |       1 |       0 |\n",
      "| 333 |       0 |       0 |       0 |\n",
      "| 334 |       0 |       0 |       1 |\n",
      "| 335 |       1 |       0 |       0 |\n",
      "| 336 |       0 |       0 |       0 |\n",
      "| 337 |       0 |       1 |       0 |\n",
      "| 338 |       0 |       1 |       0 |\n",
      "| 339 |       1 |       1 |       0 |\n",
      "| 340 |       1 |       0 |       0 |\n",
      "| 341 |       0 |       0 |     nan |\n",
      "| 342 |       0 |       0 |       1 |\n",
      "| 343 |       1 |       0 |       0 |\n",
      "| 344 |       0 |       0 |       0 |\n",
      "| 345 |       1 |       0 |       0 |\n",
      "| 346 |       1 |       0 |       0 |\n",
      "| 347 |       1 |       1 |       0 |\n",
      "| 348 |       0 |       1 |       0 |\n",
      "| 349 |       1 |       0 |       1 |\n",
      "| 350 |       0 |       0 |       1 |\n",
      "| 351 |       1 |       0 |       0 |\n",
      "| 352 |       0 |       0 |       1 |\n",
      "| 353 |       1 |       0 |       0 |\n",
      "| 354 |       0 |       0 |       1 |\n",
      "| 355 |       1 |       1 |       0 |\n",
      "| 356 |       1 |       0 |       0 |\n",
      "| 357 |       1 |       0 |       0 |\n",
      "| 358 |       0 |       1 |       1 |\n",
      "| 359 |       1 |       0 |       0 |\n",
      "| 360 |       0 |       1 |       0 |\n",
      "| 361 |       0 |       1 |       0 |\n",
      "| 362 |       0 |       1 |       1 |\n",
      "| 363 |       1 |       1 |       1 |\n",
      "| 364 |       1 |       1 |       1 |\n",
      "| 365 |       0 |       0 |       1 |\n",
      "| 366 |       0 |       1 |       0 |\n",
      "| 367 |       0 |       1 |       1 |\n",
      "| 368 |       0 |       0 |       1 |\n",
      "| 369 |       1 |       0 |       1 |\n",
      "| 370 |       0 |       1 |       1 |\n",
      "| 371 |       0 |       1 |       1 |\n",
      "| 372 |       0 |       0 |       1 |\n",
      "| 373 |       0 |       0 |       1 |\n",
      "| 374 |       1 |       0 |       1 |\n",
      "| 375 |       0 |       0 |       1 |\n",
      "| 376 |       0 |       1 |     nan |\n",
      "| 377 |       1 |       0 |       1 |\n",
      "| 378 |       1 |       0 |       0 |\n",
      "| 379 |       1 |       0 |       0 |\n",
      "| 380 |       0 |       1 |       1 |\n",
      "| 381 |       0 |       1 |       1 |\n",
      "| 382 |       0 |       1 |       1 |\n",
      "| 383 |       0 |       1 |       1 |\n",
      "| 384 |       0 |       1 |       1 |\n",
      "| 385 |       0 |       1 |       1 |\n",
      "| 386 |       0 |       0 |     nan |\n",
      "| 387 |       0 |       1 |       1 |\n",
      "| 388 |       0 |       0 |       1 |\n",
      "| 389 |       0 |       1 |       0 |\n",
      "| 390 |       0 |     nan |     nan |\n",
      "| 391 |       0 |       1 |       1 |\n",
      "| 392 |       1 |       1 |       1 |\n",
      "| 393 |       1 |     nan |     nan |\n",
      "| 394 |       0 |       1 |       1 |\n",
      "| 395 |       0 |       1 |       1 |\n",
      "| 396 |       0 |       1 |       1 |\n",
      "| 397 |       0 |       1 |       1 |\n",
      "| 398 |       0 |       0 |       1 |\n",
      "| 399 |       1 |       0 |       1 |\n",
      "| 400 |       1 |       0 |       1 |\n",
      "| 401 |       1 |       0 |       1 |\n",
      "| 402 |       1 |     nan |       0 |\n",
      "| 403 |       1 |       0 |       1 |\n",
      "| 404 |       1 |       1 |       1 |\n",
      "| 405 |       1 |       0 |       0 |\n",
      "| 406 |       0 |       1 |       0 |\n",
      "| 407 |       0 |       0 |       0 |\n",
      "| 408 |       0 |       1 |       0 |\n",
      "| 409 |       1 |       0 |       0 |\n",
      "| 410 |       1 |       0 |       0 |\n",
      "| 411 |       0 |       1 |       0 |\n",
      "| 412 |       1 |       0 |       0 |\n",
      "| 413 |       1 |       1 |       1 |\n",
      "| 414 |       0 |       1 |       1 |\n",
      "| 415 |       0 |       0 |       1 |\n",
      "| 416 |       1 |       1 |       1 |\n",
      "| 417 |       0 |       1 |       1 |\n",
      "| 418 |       0 |       1 |       1 |\n",
      "| 419 |       0 |       1 |       1 |\n",
      "| 420 |       1 |       1 |       1 |\n",
      "| 421 |       0 |       0 |       1 |\n",
      "| 422 |       0 |       0 |       0 |\n",
      "| 423 |       0 |       0 |       1 |\n",
      "| 424 |       0 |       0 |       1 |\n",
      "| 425 |       0 |       0 |       0 |\n",
      "| 426 |       0 |       1 |       0 |\n",
      "| 427 |       1 |       0 |       0 |\n",
      "| 428 |       0 |     nan |     nan |\n",
      "| 429 |       0 |       1 |       0 |\n",
      "| 430 |       1 |       0 |       0 |\n",
      "| 431 |       0 |       0 |       0 |\n",
      "| 432 |       1 |       0 |     nan |\n",
      "| 433 |       1 |       0 |       0 |\n",
      "| 434 |       1 |       0 |       1 |\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code generates all 6 possible **linear chain DAGs** for 3 variables (`0,1,2`):\n",
    "1. **Permutations**: Creates every node order (e.g., `0→1→2`, `1→0→2`).\n",
    "2. **Parent Assignment**: Each node (except first) gets the prior node as its parent.\n",
    "3. **Output**: List of DAGs like `{0:[], 1:[0], 2:[1]}`, representing causal chains."
   ],
   "id": "ddad668dbffd39ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:48.764871Z",
     "start_time": "2025-05-06T18:00:48.761729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dags = []\n",
    "for perm in permutations([0, 1, 2]):\n",
    "    parents = {perm[0]: [], perm[1]: [perm[0]], perm[2]: [perm[1]]}\n",
    "    dags.append(parents)"
   ],
   "id": "eabb926bb74ad3c6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:49.349228Z",
     "start_time": "2025-05-06T18:00:49.344395Z"
    }
   },
   "cell_type": "code",
   "source": "print(dags)",
   "id": "a2bbdb6475e0231a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: [], 1: [0], 2: [1]}, {0: [], 2: [0], 1: [2]}, {1: [], 0: [1], 2: [0]}, {1: [], 2: [1], 0: [2]}, {2: [], 0: [2], 1: [0]}, {2: [], 1: [2], 0: [1]}]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code initializes **Conditional Probability Tables (CPTs)** for a Bayesian network:\n",
    "1. **Seed Control**: Sets random seed for reproducibility.\n",
    "2. **Root Nodes**: Initializes root node CPTs (`[P(0), P(1)]`) using a uniform Dirichlet distribution.\n",
    "3. **Child Nodes**: Creates CPT arrays for all parent configurations (e.g., `2^len(parents)` rows), each row being a Dirichlet-sampled distribution."
   ],
   "id": "5ae73f1015958408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:51.149568Z",
     "start_time": "2025-05-06T18:00:51.145510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_cpts(dag, seed):\n",
    "    np.random.seed(seed)\n",
    "    cpts = {}\n",
    "    for node in range(3):\n",
    "        parents = dag[node]\n",
    "        if not parents:\n",
    "            cpts[node] = np.random.dirichlet([1, 1])\n",
    "        else:\n",
    "            num_parent_configs = 2 ** len(parents)\n",
    "            cpts[node] = np.random.dirichlet([1, 1], size=num_parent_configs)\n",
    "    return cpts"
   ],
   "id": "e0999da351df4f5e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code initializes **expected counts** for EM's E-step:\n",
    "1. **Structure**: Creates a dictionary `(node, parent_config, value) → count`.\n",
    "2. **Parent Handling**: For nodes with parents, initializes counts for all `0/1` parent combinations.\n",
    "3. **Root Nodes**: Initializes counts for root nodes without parents.\n",
    "All counts start at `0.0` before accumulating probabilities."
   ],
   "id": "9092bdf0677d61ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:53.241811Z",
     "start_time": "2025-05-06T18:00:53.238183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_expected_counts(dag):\n",
    "    expected_counts = defaultdict(float)\n",
    "    for node in range(3):\n",
    "        parents = dag[node]\n",
    "        if parents:\n",
    "            for p_combo in product([0, 1], repeat=len(parents)):\n",
    "                expected_counts[(node, p_combo, 1)] = 0.0\n",
    "                expected_counts[(node, p_combo, 0)] = 0.0\n",
    "        else:\n",
    "            expected_counts[(node, (), 1)] = 0.0\n",
    "            expected_counts[(node, (), 0)] = 0.0\n",
    "    return expected_counts"
   ],
   "id": "b8b8a878abad0f44",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code processes **complete samples** for the EM algorithm:\n",
    "1. **Delta Tracking**: Counts `(node, parent_config, value)` occurrences.\n",
    "2. **Log-Likelihood**: Computes sample probability using CPTs.\n",
    "3. **Parent Handling**: Converts parent values to a CPT index via binary encoding.\n",
    "4. **Probability Access**: Uses node value (`0/1`) to index CPT rows."
   ],
   "id": "1730008e17cffc7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:55.057687Z",
     "start_time": "2025-05-06T18:00:55.052599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_complete_sample(sample_vals, dag, cpts):\n",
    "    delta = defaultdict(float)\n",
    "    log_likelihood = 0.0\n",
    "    for node in range(3):\n",
    "        parents = dag[node]\n",
    "        val = int(sample_vals[node])\n",
    "\n",
    "        if parents:\n",
    "            p_vals = tuple(sample_vals[parents].astype(int))\n",
    "            parent_config_idx = sum(p * (2 ** i) for i, p in enumerate(p_vals))\n",
    "            prob_dist = cpts[node][parent_config_idx]\n",
    "        else:\n",
    "            prob_dist = cpts[node]\n",
    "\n",
    "        p = prob_dist[val]\n",
    "        log_likelihood += np.log(p)\n",
    "\n",
    "        key = (node, p_vals if parents else (), val)\n",
    "        delta[key] += 1\n",
    "    return log_likelihood, delta"
   ],
   "id": "a7754fb41b8cb822",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Function Purpose**: Handles samples with missing values in the EM algorithm's E-step.\n",
    "1. **Imputation**: Generates all possible `0/1` combinations for missing variables.\n",
    "2. **Joint Probability**: Computes the probability of each imputed sample using CPTs.\n",
    "3. **Normalization**: Weights each imputation by its probability (soft assignment).\n",
    "4. **Delta Update**: Accumulates fractional counts (`(node, parent_config, value) → count`) for CPT updates.\n",
    "5. **Log-Likelihood**: Computes the weighted log-probability of the sample.\n",
    "**Key Step**: Uses binary encoding (`parent_config_index`) to access CPT rows for parent value combinations."
   ],
   "id": "7a8fce11a0491f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:57.298992Z",
     "start_time": "2025-05-06T18:00:57.291225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_incomplete_sample(sample_vals, dag, cpts, missing_vars):\n",
    "    delta = defaultdict(float)\n",
    "    log_likelihood = 0.0\n",
    "    possible_combos = list(product([0, 1], repeat=len(missing_vars)))\n",
    "    combo_probs, total_prob = [], 0.0\n",
    "\n",
    "    for combo in possible_combos:\n",
    "        imputed = sample_vals.copy()\n",
    "        for i, var in enumerate(missing_vars):\n",
    "            imputed[var] = combo[i]\n",
    "\n",
    "        joint_prob = 1.0\n",
    "        for node in range(3):\n",
    "            parents = dag[node]\n",
    "            val = int(imputed[node])\n",
    "\n",
    "            if parents:\n",
    "                p_vals = tuple(imputed[parents].astype(int))\n",
    "                parent_config_index = sum([p * (2 ** i) for i, p in enumerate(p_vals)])\n",
    "                prob_dist = cpts[node][parent_config_index]\n",
    "            else:\n",
    "                prob_dist = cpts[node]\n",
    "\n",
    "            p = prob_dist[val]\n",
    "            joint_prob *= p\n",
    "\n",
    "        combo_probs.append(joint_prob)\n",
    "        total_prob += joint_prob\n",
    "\n",
    "    if total_prob == 0:\n",
    "        return 0.0, delta\n",
    "\n",
    "    for combo, joint_prob in zip(possible_combos, combo_probs):\n",
    "        prob = joint_prob / total_prob\n",
    "        imputed = sample_vals.copy()\n",
    "        for i, var in enumerate(missing_vars):\n",
    "            imputed[var] = combo[i]\n",
    "\n",
    "        for node in range(3):\n",
    "            parents = dag[node]\n",
    "            val = int(imputed[node])\n",
    "\n",
    "            if parents:\n",
    "                p_vals = tuple(imputed[parents].astype(int))\n",
    "            else:\n",
    "                p_vals = ()\n",
    "\n",
    "            key = (node, p_vals, val)\n",
    "            delta[key] += prob\n",
    "\n",
    "        log_likelihood += prob * np.log(joint_prob)\n",
    "\n",
    "    return log_likelihood, delta"
   ],
   "id": "8c941672b1a58d2d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Function Purpose**: Executes the **E-step** of the EM algorithm for Bayesian networks.\n",
    "1. **Initialization**: Creates `expected_counts` to track node-parent-value occurrences.\n",
    "2. **Sample Processing**: For each sample:\n",
    "   - **Complete Data**: Directly computes counts/log-likelihood via `process_complete_sample`.\n",
    "   - **Missing Data**: Imputes missing values probabilistically via `process_incomplete_sample`.\n",
    "3. **Aggregation**: Accumulates log-likelihood and updates `expected_counts` for CPT estimation.\n",
    "**Output**: Returns `expected_counts` (for M-step CPT updates) and total `log_likelihood` (for convergence checks)."
   ],
   "id": "af6ffc810d3603f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:00:58.996421Z",
     "start_time": "2025-05-06T18:00:58.992589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perform_e_step(dag, train_data, cpts):\n",
    "    expected_counts = initialize_expected_counts(dag)\n",
    "    log_likelihood = 0.0\n",
    "    for _, sample in train_data.iterrows():\n",
    "        sample_vals = sample.to_numpy()\n",
    "        missing = [i for i, val in enumerate(sample_vals) if np.isnan(val)]\n",
    "        if not missing:\n",
    "            ll, delta = process_complete_sample(sample_vals, dag, cpts)\n",
    "        else:\n",
    "            ll, delta = process_incomplete_sample(sample_vals, dag, cpts, missing)\n",
    "        log_likelihood += ll\n",
    "        for key, value in delta.items():\n",
    "            expected_counts[key] += value\n",
    "    return expected_counts, log_likelihood"
   ],
   "id": "6998e2196cb5d2e7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Function Purpose**: Performs the **M-step** of the EM algorithm to update CPTs.\n",
    "1. **Root Nodes**:\n",
    "   - Computes `[P(node=0), P(node=1)]` from counts.\n",
    "   - Uses Laplace smoothing (`[0.5, 0.5]`) if no data.\n",
    "2. **Child Nodes**:\n",
    "   - For each parent configuration (e.g., `(0,1)`):\n",
    "     - Converts parent values to an index via binary encoding.\n",
    "     - Computes `[P(node=0|parents), P(node=1|parents)]` from counts.\n",
    "     - Applies Laplace smoothing if no data for the configuration.\n",
    "3. **Output**: Returns updated CPTs as numpy arrays for efficient access.\n",
    "**Key Feature**: Ensures valid probabilities even with missing data (no division by zero)."
   ],
   "id": "d0ecddac9c4d06a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:01:01.074255Z",
     "start_time": "2025-05-06T18:01:01.068163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perform_m_step(expected_counts, dag):\n",
    "    new_cpts = {}\n",
    "    for node in range(3):\n",
    "        parents = dag[node]\n",
    "        if not parents:\n",
    "            count_0 = expected_counts.get((node, (), 0), 0)\n",
    "            count_1 = expected_counts.get((node, (), 1), 0)\n",
    "            total = count_0 + count_1\n",
    "            if total == 0:\n",
    "                new_cpt = np.array([0.5, 0.5])\n",
    "            else:\n",
    "                new_cpt = np.array([count_0 / total, count_1 / total])\n",
    "            new_cpts[node] = new_cpt\n",
    "        else:\n",
    "            num_parent_configs = 2 ** len(parents)\n",
    "            new_cpt = np.zeros((num_parent_configs, 2))\n",
    "            for parent_config in product([0, 1], repeat=len(parents)):\n",
    "                parent_config_idx = sum(p * (2 ** i) for i, p in enumerate(parent_config))\n",
    "                count_0 = expected_counts.get((node, parent_config, 0), 0)\n",
    "                count_1 = expected_counts.get((node, parent_config, 1), 0)\n",
    "                total = count_0 + count_1\n",
    "                if total == 0:\n",
    "                    new_cpt[parent_config_idx] = [0.5, 0.5]\n",
    "                else:\n",
    "                    new_cpt[parent_config_idx] = [count_0 / total, count_1 / total]\n",
    "            new_cpts[node] = new_cpt\n",
    "    return new_cpts"
   ],
   "id": "452d5fe21454d3e3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Function Purpose**: Implements the **EM algorithm** to learn CPTs for a Bayesian network.\n",
    "1. **Initialization**: Generates random CPTs using `initialize_cpts`.\n",
    "2. **EM Loop**:\n",
    "   - **E-Step**: Computes expected counts and log-likelihood using current CPTs.\n",
    "   - **M-Step**: Updates CPTs using expected counts.\n",
    "   - **Convergence Check**: Stops if log-likelihood change is below `epsilon`.\n",
    "3. **Termination**: Returns learned CPTs after convergence or reaching `max_iters`.\n",
    "**Key Features**:\n",
    "- Handles missing data via probabilistic imputation (E-step).\n",
    "- Guarantees convergence by monitoring log-likelihood improvement.\n",
    "- Seed ensures reproducible CPT initialization."
   ],
   "id": "f38a03f46186d289"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:01:05.189351Z",
     "start_time": "2025-05-06T18:01:05.185292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def learn_em(dag, train_data, max_iters=100, epsilon=1e-3, seed=None):\n",
    "    cpts = initialize_cpts(dag, seed)\n",
    "    prev_log_likelihood = -np.inf\n",
    "    for _ in range(max_iters):\n",
    "        expected_counts, log_likelihood = perform_e_step(dag, train_data, cpts)\n",
    "        new_cpts = perform_m_step(expected_counts, dag)\n",
    "        if np.abs(log_likelihood - prev_log_likelihood) < epsilon:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "        cpts = new_cpts\n",
    "    return cpts"
   ],
   "id": "dffe5c3301e1f643",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train models for each DAG",
   "id": "7e866da5e2d1f244"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:01:07.340070Z",
     "start_time": "2025-05-06T18:01:06.915977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = []\n",
    "for dag in dags:\n",
    "    cpts = learn_em(dag, train_data)\n",
    "    models.append((dag, cpts))"
   ],
   "id": "2c13bc3b3f03bd9c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Function Purpose**: Compares two sets of CPTs to check for convergence or equivalence.\n",
    "1. **Node-wise Check**: Iterates through all nodes in the Bayesian network.\n",
    "2. **Array Handling**: For array-based CPTs (child nodes), uses `np.allclose` to compare element-wise within `tol`.\n",
    "3. **Scalar Handling**: For scalar CPTs (root nodes), checks absolute difference.\n",
    "4. **Tolerance**: Allows slight numerical differences (e.g., `tol=0.01`).\n",
    "**Returns**: `True` if all CPTs match within tolerance, `False` otherwise."
   ],
   "id": "2c41d2815790420b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:01:08.832904Z",
     "start_time": "2025-05-06T18:01:08.829477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_cpts(cpts1, cpts2, tol=1e-2):\n",
    "    for node in cpts1:\n",
    "        if isinstance(cpts1[node], np.ndarray):\n",
    "            if not np.allclose(cpts1[node], cpts2[node], atol=tol):\n",
    "                return False\n",
    "        else:\n",
    "            if abs(cpts1[node] - cpts2[node]) > tol:\n",
    "                return False\n",
    "    return True"
   ],
   "id": "e8d9da01535894c4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Code Purpose**: Tests if EM algorithm produces different models across runs.\n",
    "1. **Multiple Runs**: Trains models on 20% masked data with different seeds.\n",
    "2. **CPT Storage**: Saves learned CPTs for each run.\n",
    "3. **CPT Comparison**: Checks if CPTs differ between runs using `compare_cpts`.\n",
    "**Key Insights**:\n",
    "- **Initialization Sensitivity**: Different seeds → different CPT initializations/missing data masks.\n",
    "- **Local Optima**: EM may converge to different CPTs if likelihood has multiple peaks.\n",
    "**Output**: Prints whether models differ (Yes/No), confirming EM's initialization dependence."
   ],
   "id": "26312e51c311c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:01:13.058880Z",
     "start_time": "2025-05-06T18:01:12.736962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runs = []\n",
    "for seed in [42, 99, 123, 7, 2023]:\n",
    "    np.random.seed(seed)\n",
    "    mask = np.random.rand(*train_data.shape) < 0.20\n",
    "    train_data_masked = train_data.mask(mask)\n",
    "    cpts = learn_em(dags[0], train_data_masked, seed=seed)\n",
    "    runs.append(cpts)\n",
    "    print(f\"\\n--- Run {seed} CPTs ---\")\n",
    "    for node in cpts:\n",
    "        print(f\"Node {node}: {cpts[node]}\")\n",
    "\n",
    "print(\"\\nDo different EM runs produce different models?\")\n",
    "all_same = True\n",
    "for i in range(len(runs)):\n",
    "    for j in range(i + 1, len(runs)):\n",
    "        if not compare_cpts(runs[i], runs[j]):\n",
    "            all_same = False\n",
    "            print(f\"Run {i + 1} vs Run {j + 1}: Different CPTs\")\n",
    "if all_same:\n",
    "    print(\"All runs produced identical models\")\n",
    "else:\n",
    "    print(\"Conclusion: Yes, different runs yield different models\")"
   ],
   "id": "9d1e64c712852554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 42 CPTs ---\n",
      "Node 0: [0.62333333 0.37666667]\n",
      "Node 1: [[0.39454685 0.60545315]\n",
      " [0.8125     0.1875    ]]\n",
      "Node 2: [[0.4982726  0.5017274 ]\n",
      " [0.53159143 0.46840857]]\n",
      "\n",
      "--- Run 99 CPTs ---\n",
      "Node 0: [0.62333333 0.37666667]\n",
      "Node 1: [[0.39454369 0.60545631]\n",
      " [0.8125     0.1875    ]]\n",
      "Node 2: [[0.49858633 0.50141367]\n",
      " [0.53169426 0.46830574]]\n",
      "\n",
      "--- Run 123 CPTs ---\n",
      "Node 0: [0.62333333 0.37666667]\n",
      "Node 1: [[0.39453932 0.60546068]\n",
      " [0.8125     0.1875    ]]\n",
      "Node 2: [[0.49855713 0.50144287]\n",
      " [0.53158486 0.46841514]]\n",
      "\n",
      "--- Run 7 CPTs ---\n",
      "Node 0: [0.62333333 0.37666667]\n",
      "Node 1: [[0.39454134 0.60545866]\n",
      " [0.8125     0.1875    ]]\n",
      "Node 2: [[0.49847749 0.50152251]\n",
      " [0.53157879 0.46842121]]\n",
      "\n",
      "--- Run 2023 CPTs ---\n",
      "Node 0: [0.62333333 0.37666667]\n",
      "Node 1: [[0.39454834 0.60545166]\n",
      " [0.8125     0.1875    ]]\n",
      "Node 2: [[0.49825746 0.50174254]\n",
      " [0.53161258 0.46838742]]\n",
      "\n",
      "Do different EM runs produce different models?\n",
      "All runs produced identical models\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Code Purpose**: Evaluates Bayesian network models on test data to compute accuracy.\n",
    "1. **Loop Over Models**: Tests each DAG structure (`dag`) and its learned CPTs (`cpts`).\n",
    "2. **Test Sample Processing**: For each test sample:\n",
    "   - Extracts observed votes (`vote1`, `vote2`) and true `party`.\n",
    "   - Ignores samples with missing `party`.\n",
    "3. **Probability Calculation**:\n",
    "   - **Party Node (0)**: Uses parent configuration (if any) to fetch CPT probabilities `[P(0), P(1)]`.\n",
    "   - **Vote Nodes (1,2)**: Multiplies probabilities of observed votes given their parent configurations.\n",
    "4. **Prediction**: Predicts `party` using normalized probabilities (`prob_1 / total_prob > 0.5`).\n",
    "5. **Accuracy**: Computes accuracy as `correct / total` and stores results.\n",
    "**Key Mechanism**: Uses parent configuration indexing (`parent_config_idx`) to access CPTs efficiently.\n",
    "**Output**: List of `(dag, accuracy)` pairs showing how well each DAG structure predicts party affiliation."
   ],
   "id": "8cc629b4016082d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:03:07.037475Z",
     "start_time": "2025-05-06T18:03:07.003831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for dag, cpts in models:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, test_sample in test_data.iterrows():\n",
    "        party = test_sample['party']\n",
    "        if np.isnan(party):\n",
    "            continue\n",
    "\n",
    "        vote1 = test_sample['vote1']\n",
    "        vote2 = test_sample['vote2']\n",
    "        observed = {}\n",
    "        if not np.isnan(vote1):\n",
    "            observed[1] = int(vote1)\n",
    "        if not np.isnan(vote2):\n",
    "            observed[2] = int(vote2)\n",
    "\n",
    "        prob_0 = 1.0\n",
    "        prob_1 = 1.0\n",
    "\n",
    "        node = 0\n",
    "        parents = dag[node]\n",
    "        if parents:\n",
    "            p_vals = tuple([observed.get(p, 0) for p in parents])\n",
    "            parent_config_idx = sum(p * (2 ** i) for i, p in enumerate(p_vals))\n",
    "            prob_1 *= cpts[node][parent_config_idx][1]\n",
    "            prob_0 *= cpts[node][parent_config_idx][0]\n",
    "        else:\n",
    "            prob_1 *= cpts[node][1]\n",
    "            prob_0 *= cpts[node][0]\n",
    "\n",
    "        for node in [1, 2]:\n",
    "            if node not in observed:\n",
    "                continue\n",
    "            val = observed[node]\n",
    "            parents = dag[node]\n",
    "            if parents:\n",
    "                p_vals = tuple([observed.get(p, 0) for p in parents])\n",
    "                parent_config_idx = sum(p * (2 ** i) for i, p in enumerate(p_vals))\n",
    "                p = cpts[node][parent_config_idx][val]\n",
    "            else:\n",
    "                p = cpts[node][val]\n",
    "\n",
    "            prob_1 *= p\n",
    "            prob_0 *= p\n",
    "\n",
    "        total_prob = prob_0 + prob_1\n",
    "        if total_prob == 0:\n",
    "            predicted = 0\n",
    "        else:\n",
    "            predicted = 1 if (prob_1 / total_prob) > 0.5 else 0\n",
    "        correct += (predicted == party)\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total else 0\n",
    "    results.append((dag, accuracy))"
   ],
   "id": "cb0558fa7479de92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG: {0: [], 1: [0], 2: [1]}, Accuracy: 0.6815\n",
      "DAG: {0: [], 2: [0], 1: [2]}, Accuracy: 0.5926\n",
      "DAG: {1: [], 0: [1], 2: [0]}, Accuracy: 0.6815\n",
      "DAG: {1: [], 2: [1], 0: [2]}, Accuracy: 0.5926\n",
      "DAG: {2: [], 0: [2], 1: [0]}, Accuracy: 0.6815\n",
      "DAG: {2: [], 1: [2], 0: [1]}, Accuracy: 0.6815\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T02:43:47.314393Z",
     "start_time": "2025-05-06T02:43:47.306262Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG: {0: [], 1: [0], 2: [1]}, Accuracy: 0.5926\n",
      "DAG: {0: [], 2: [0], 1: [2]}, Accuracy: 0.5926\n",
      "DAG: {1: [], 0: [1], 2: [0]}, Accuracy: 0.6815\n",
      "DAG: {1: [], 2: [1], 0: [2]}, Accuracy: 0.5926\n",
      "DAG: {2: [], 0: [2], 1: [0]}, Accuracy: 0.5926\n",
      "DAG: {2: [], 1: [2], 0: [1]}, Accuracy: 0.6815\n"
     ]
    }
   ],
   "execution_count": 230,
   "source": [
    "for dag, acc in results:\n",
    "    print(f\"DAG: {dag}, Accuracy: {acc:.4f}\")"
   ],
   "id": "57a9f2a024c46faf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
